{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import operator\n",
    "import rake as rake\n",
    "from datetime import datetime\n",
    "rake_object = rake.Rake(\"SmartStoplist.txt\", 1, 2, 1)\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function in run_praw.py, so I'm using it as a reference for all my algs\n",
    "def display_praw(name, num):\n",
    "    reddit = praw.Reddit(client_id='Pj5o8QpNXXJY9A',\n",
    "                         client_secret='pQKMRBmhp0In48NoNvvktfRo2eA',\n",
    "                         password='prawisgreat',\n",
    "                         user_agent='Reddit Unlocked CS196 Project @ UIUC',\n",
    "                         username='RedditUnlocked196')\n",
    "\n",
    "    subreddit = reddit.subreddit(name)\n",
    "\n",
    "    threads_df = pd.DataFrame({\n",
    "        'Title': (),\n",
    "        'URL': (),\n",
    "        'Upvote Ratio (%)': (),\n",
    "        'Net Score': (),\n",
    "        '# of Upvotes': (),\n",
    "        '# of Downvotes': (),\n",
    "        'Post Date': (),\n",
    "        'Self Post?': (),\n",
    "        'Video Post?': (),\n",
    "        'Domain': ()\n",
    "    })\n",
    "\n",
    "    threads_df = threads_df[['Title', 'URL', 'Upvote Ratio (%)', 'Net Score', '# of Upvotes', '# of Downvotes',\n",
    "                             'Post Date', 'Self Post?', 'Video Post?', 'Domain']]\n",
    "\n",
    "    for thread in subreddit.top('year', limit=num): # TODO: change limit number when actually deploying program. 15 is the testing number.\n",
    "        actualUps = int((thread.upvote_ratio * thread.score) / (thread.upvote_ratio * 2 - 1))\n",
    "        actualDowns = actualUps - thread.score\n",
    "        gather = pd.Series([thread.title, thread.url, thread.upvote_ratio * 100, thread.score,\n",
    "                            actualUps, actualDowns, thread.created_utc,\n",
    "                            thread.is_self, thread.is_video, thread.domain],\n",
    "                           index=['Title', 'URL', 'Upvote Ratio (%)', 'Net Score', '# of Upvotes', '# of Downvotes',\n",
    "                                  'Post Date', 'Self Post?', 'Video Post?', 'Domain'])\n",
    "        threads_df = threads_df.append(gather, ignore_index=True)\n",
    "\n",
    "    threads_dict = threads_df.to_dict(orient='records')\n",
    "\n",
    "    for entry in threads_dict:\n",
    "        if isinstance(str(entry['Post Date']), str):\n",
    "            time = datetime.fromtimestamp(entry['Post Date'])\n",
    "            formatTime = time.strftime('%b %d, %Y')\n",
    "        else:\n",
    "            formatTime = None\n",
    "\n",
    "        entry['Post Date'] = formatTime\n",
    "\n",
    "    return threads_dict\n",
    "\n",
    "\n",
    "def get_keyword_dict(num):\n",
    "    # Transforms dict returned by display_praw into DataFrame for working with\n",
    "    top10news_df = pd.DataFrame.from_dict(display_praw('news', num))\n",
    "\n",
    "    words = {}\n",
    "\n",
    "    ## NEWSPAPER STUFF HERE ##\n",
    "\n",
    "    # Get keywords out of all articles\n",
    "    for i in range(len(top10news_df)):\n",
    "        #top10news_df.iloc[i]['url']\n",
    "        myArticle = Article(top10news_df.iloc[i]['URL'])\n",
    "        myArticle.download()\n",
    "        myArticle.parse()\n",
    "        myArticle.nlp()\n",
    "\n",
    "        # Run sentiment analysis on each article, fetch subjectivity and polarity\n",
    "        text = myArticle.text\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "        # Get associated Reddit post info for each keyword, store in dictionary\n",
    "        for keyword in myArticle.keywords:\n",
    "\n",
    "            # Don't waste time with numeric keywords, skip them if they contain numbers\n",
    "            if any(char.isdigit() for char in keyword):\n",
    "                continue        \n",
    "\n",
    "            if keyword not in words:\n",
    "                words[keyword] = [keyword, 1, \n",
    "                                  top10news_df.iloc[i]['# of Upvotes'],\n",
    "                                  top10news_df.iloc[i][\"# of Downvotes\"], \n",
    "                                  top10news_df.iloc[i][\"Net Score\"],\n",
    "                                  subjectivity, polarity, \n",
    "                                  {(top10news_df.iloc[i][\"Domain\"]):1}]\n",
    "            else:\n",
    "                words[keyword][1] += 1\n",
    "                words[keyword][2] += top10news_df.iloc[i]['# of Upvotes']\n",
    "                words[currentWord][3] += int(top10news_df.iloc[i]['# of Downvotes'])\n",
    "                words[currentWord][4] += int(top10news_df.iloc[i]['Net Score'])\n",
    "                words[currentWord][5] = np.mean([subjectivity, words[currentWord][5]])\n",
    "                words[currentWord][6] = np.mean([polarity, words[currentWord][6]])\n",
    "                if top10news_df.iloc[i][\"Domain\"] in words[currentWord][7]:\n",
    "                    words[currentWord][7][(top10news_df.iloc[i][\"Domain\"])] += 1\n",
    "                else:\n",
    "                    words[currentWord][7][top10news_df.iloc[i][\"Domain\"]] = 1\n",
    "\n",
    "        ## RAKE STUFF HERE ##\n",
    "\n",
    "        # Pull keywords from title strings\n",
    "        for wordPair in rake_object.run(top10news_df.iloc[i]['Title']):\n",
    "            currentWord = wordPair[0]\n",
    "\n",
    "            # Don't waste time with numeric keywords, skip them if they contain numbers\n",
    "            if any(char.isdigit() for char in currentWord):\n",
    "                continue\n",
    "\n",
    "            # Grab associated Reddit post data for each keyword, store in dictionary\n",
    "            if currentWord not in words:\n",
    "                words[currentWord] = [currentWord, 1, \n",
    "                                  top10news_df.iloc[i]['# of Upvotes'],\n",
    "                                  top10news_df.iloc[i][\"# of Downvotes\"], \n",
    "                                  top10news_df.iloc[i][\"Net Score\"],\n",
    "                                  subjectivity, polarity, \n",
    "                                  {(top10news_df.iloc[i][\"Domain\"]):1}]\n",
    "            else:\n",
    "                words[currentWord][1] += 1\n",
    "                words[currentWord][2] += int(top10news_df.iloc[i]['# of Upvotes'])\n",
    "                words[currentWord][3] += int(top10news_df.iloc[i]['# of Downvotes'])\n",
    "                words[currentWord][4] += int(top10news_df.iloc[i]['Net Score'])\n",
    "                if top10news_df.iloc[i][\"Domain\"] in words[currentWord][7]:\n",
    "                    words[currentWord][7][(top10news_df.iloc[i][\"Domain\"])] += 1\n",
    "                else:\n",
    "                    words[currentWord][7][top10news_df.iloc[i][\"Domain\"]] = 1\n",
    "\n",
    "\n",
    "    ### FOR GARY'S USE ###\n",
    "    # Output dictionary is named 'words' #\n",
    "    # Format is as such: #\n",
    "    # key = keyword #\n",
    "    # value = [Occurences, Upvotes, Downvotes, Score, Subjectivity, Polarity, Domain Dictionary] #\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article `download()` failed with 410 Client Error: Gone for url: https://www.nytimes.com/aponline/2017/10/13/us/ap-us-puppy-mills-california.html on URL https://www.nytimes.com/aponline/2017/10/13/us/ap-us-puppy-mills-california.html\n"
     ]
    },
    {
     "ename": "ArticleException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-772cbcfeea7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mget_keyword_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtimings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c6a4f22e4a59>\u001b[0m in \u001b[0;36mget_keyword_dict\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmyArticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop10news_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmyArticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mmyArticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mmyArticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/newspaper3k-0.2.5-py3.6.egg/newspaper/article.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow_if_not_downloaded_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/newspaper3k-0.2.5-py3.6.egg/newspaper/article.py\u001b[0m in \u001b[0;36mthrow_if_not_downloaded_verbose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m             print('Article `download()` failed with %s on URL %s' %\n\u001b[1;32m    524\u001b[0m                   (self.download_exception_msg, self.url))\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mArticleException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthrow_if_not_parsed_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArticleException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import csv\n",
    "\n",
    "# Runtime comparison\n",
    "original_start = datetime.now()\n",
    "\n",
    "timings = []\n",
    "for i in range(5, 105, 5):\n",
    "    start_time = datetime.now()\n",
    "    get_keyword_dict(i)\n",
    "    time_taken = datetime.now() - start_time\n",
    "    timings.append((i, time_taken))\n",
    "    with open(\"times.csv\", 'a') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([i, time_taken])\n",
    "\n",
    "final = datetime.now() - original_start\n",
    "print(\"Total time taken: %d\" % final)\n",
    "\n",
    "for pair in timings:\n",
    "    plt.scatter(pair[0], pair[1])\n",
    "    plt.plot(pair[0], pair[1])\n",
    "plt.xlabel(\"Number of Posts Parsed\")\n",
    "plt.ylabel(\"Processing Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
